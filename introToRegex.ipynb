{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYT6U8CGJZ3g"
      },
      "source": [
        "# מטלה מספר 5\n",
        "\n",
        "# REGEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MWpeQ1x1l3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796dabfd-ea01-4f5c-c4b8-821d62ee9325"
      },
      "source": [
        "#Create a folder in google's cloud for my stuff\n",
        "!mkdir /root/.kaggle/ "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ-aIfCMKZkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f619b77-1098-4beb-a82c-8d1c963c9dc6"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Installing the Kaggle package\n",
        "!pip install kaggle \n",
        "\n",
        "#Important Note: complete this with your own key - after running this for the first time remmember to **remove** your API_KEY\n",
        "api_token = {\"username\":\"shaharoded\",\"key\":\"47649d0e189bb2974572b495b853facb\"}\n",
        "\n",
        "# creating kaggle.json file with the personal API-Key details \n",
        "# You can also put this file on your Google Drive\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "  json.dump(api_token, file)\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kXZ1jEYKg0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095a1749-5dbd-42fb-bdca-4dc04bf79c3a"
      },
      "source": [
        "# searching for the dataset\n",
        "!kaggle datasets list -s blog"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                     title                                             size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  \n",
            "rtatman/blog-authorship-corpus                          Blog Authorship Corpus                           290MB  2017-08-15 21:20:54           6081  \n",
            "kaggle/kaggle-blog-winners-posts                        Kaggle Blog: Winners' Posts                      519KB  2016-09-21 02:21:21            730  \n",
            "lakritidis/identifying-influential-bloggers-techcrunch  Identifying Influential Bloggers: Techcrunch     112MB  2020-03-30 19:22:09            934  \n",
            "saurabhbagchi/sample-blog-corpus                        Sample Blog Corpus                               290MB  2021-02-28 12:20:26             59  \n",
            "alexgude/california-traffic-collision-data-from-switrs  California Traffic Collision Data from SWITRS      1GB  2021-08-29 16:42:35           3631  \n",
            "jessemostipak/hotel-booking-demand                      Hotel booking demand                               1MB  2020-02-13 01:27:20          61103  \n",
            "wpncrh/marginal-revolution-blog-post-data               Marginal Revolution Blog Post Data                 6MB  2016-09-18 03:26:10            241  \n",
            "kashnitsky/mlcourse                                     mlcourse.ai                                       51MB  2018-12-09 16:45:09          27594  \n",
            "clorichel/boat-types-recognition                        Boat types recognition                           842MB  2018-11-17 20:55:43           4583  \n",
            "ozgurdogan646/blog-or-not-dataset                       Blog or Not Dataset                                2MB  2021-07-05 10:56:31              9  \n",
            "vishalsubbiah/pokemon-images-and-types                  Pokemon Image Dataset                              2MB  2018-12-17 01:01:39          12998  \n",
            "crmercado/tweets-blogs-news-swiftkey-dataset-4million   Tweets Blogs News - Swiftkey Dataset 4million      1GB  2017-12-06 15:42:31            781  \n",
            "glushko/seth-godins-blogs-dataset                       Seth Godin's Blog Dataset                          4MB  2020-05-17 19:46:10             43  \n",
            "dorianlazar/medium-articles-dataset                     Medium articles dataset                            1GB  2020-06-30 14:13:56           3321  \n",
            "christianlillelund/donald-trumps-rallies                Donald Trump Rally Speeches                      720KB  2020-09-26 10:25:08           2865  \n",
            "alexattia/the-simpsons-characters-dataset               The Simpsons Characters Data                       1GB  2018-04-13 22:55:01          15959  \n",
            "xhlulu/efficientnet-keras-source-code                   EfficientNet Keras Source Code                   769KB  2021-11-10 13:21:49            644  \n",
            "fmejia21/demographics-of-academy-awards-oscars-winners  Demographics of Academy Awards (Oscars) Winners   20KB  2020-02-04 17:38:26           3926  \n",
            "kimjihoo/coronavirusdataset                             [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB  2020-07-13 14:07:31          93100  \n",
            "fournierp/captcha-version-2-images                      CAPTCHA Images                                    17MB  2019-02-27 03:04:17           8436  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx4o_evxK3o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a03fec5-bc32-4570-8d1b-f40d1d582f7a"
      },
      "source": [
        "# Creating a dataset directory\n",
        "!mkdir ./datasets\n",
        "!mkdir ./datasets/blog\n",
        "\n",
        "# download the dataset from Kaggle and unzip it\n",
        "!kaggle datasets download rtatman/blog-authorship-corpus  -f blogtext.csv -p ./datasets/blog/"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./datasets’: File exists\n",
            "Downloading blogtext.csv.zip to ./datasets/blog\n",
            " 98% 284M/290M [00:11<00:00, 23.5MB/s]\n",
            "100% 290M/290M [00:11<00:00, 27.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn6wrqXALT9Z",
        "outputId": "7322ad36-e3ff-49ab-da46-e0331436b626"
      },
      "source": [
        "#Unzip \n",
        "!unzip ./datasets/blog/blogtext.csv.zip  -d ./datasets/blog/"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./datasets/blog/blogtext.csv.zip\n",
            "  inflating: ./datasets/blog/blogtext.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjx4BJ9pLqmU"
      },
      "source": [
        "#pick libraries I want to use\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import datetime as dt"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdyPMcx_JYhD"
      },
      "source": [
        "# חלק א' - נתוני פוסטים של בלוגרים"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "d8OQn-flPw_w",
        "outputId": "70b92973-a873-47ce-998e-abf297f47e7e"
      },
      "source": [
        "#create objects to work on (tables) - paste the link of the files from the folder in the left\n",
        "blogtext = pd.read_csv(\"/content/datasets/blog/blogtext.csv\")\n",
        "blogtext"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681279</th>\n",
              "      <td>1713845</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>Student</td>\n",
              "      <td>Taurus</td>\n",
              "      <td>01,July,2004</td>\n",
              "      <td>Dear Susan,  I could write some really ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681280</th>\n",
              "      <td>1713845</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>Student</td>\n",
              "      <td>Taurus</td>\n",
              "      <td>01,July,2004</td>\n",
              "      <td>Dear Susan,  'I have the second yeast i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681281</th>\n",
              "      <td>1713845</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>Student</td>\n",
              "      <td>Taurus</td>\n",
              "      <td>01,July,2004</td>\n",
              "      <td>Dear Susan,  Your 'boyfriend' is fuckin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681282</th>\n",
              "      <td>1713845</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>Student</td>\n",
              "      <td>Taurus</td>\n",
              "      <td>01,July,2004</td>\n",
              "      <td>Dear Susan:    Just to clarify, I am as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681283</th>\n",
              "      <td>1713845</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>Student</td>\n",
              "      <td>Taurus</td>\n",
              "      <td>01,July,2004</td>\n",
              "      <td>Hey everybody...and Susan,  You might a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>681284 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  ...                                               text\n",
              "0       2059027  ...             Info has been found (+/- 100 pages,...\n",
              "1       2059027  ...             These are the team members:   Drewe...\n",
              "2       2059027  ...             In het kader van kernfusie op aarde...\n",
              "3       2059027  ...                   testing!!!  testing!!!          \n",
              "4       3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
              "...         ...  ...                                                ...\n",
              "681279  1713845  ...         Dear Susan,  I could write some really ...\n",
              "681280  1713845  ...         Dear Susan,  'I have the second yeast i...\n",
              "681281  1713845  ...         Dear Susan,  Your 'boyfriend' is fuckin...\n",
              "681282  1713845  ...         Dear Susan:    Just to clarify, I am as...\n",
              "681283  1713845  ...         Hey everybody...and Susan,  You might a...\n",
              "\n",
              "[681284 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnL4Q4REJwig"
      },
      "source": [
        "# שאלה 1\n",
        "איזה מזל משתמש הכי הרבה במילים בעלות 8 אותיות או מספרים ומעלה"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfedD_akP42u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b605b629-189e-456d-fe4d-73f904789882"
      },
      "source": [
        "#first Ill create a function that I can apply to the table\n",
        "def long_word_counter(text):\n",
        "  pattern = re.compile(\"(\\w+)\")\n",
        "  new_text_list = pattern.findall(text)\n",
        "  counter = 0\n",
        "  for word in new_text_list:\n",
        "    if len(word) >= 8:\n",
        "        counter = counter + 1\n",
        "  return counter\n",
        "#next - Ill apply my function on a copy of the table to create a new column       \n",
        "tweets = blogtext[['id','topic','sign','date','text']]\n",
        "tweets['num_of_long_words'] = tweets.text.apply(long_word_counter)\n",
        "#now that I have a table with the amount of long words each used, Ill use groupby table\n",
        "tweets_groupby = tweets.groupby(tweets['sign'],as_index = False)['num_of_long_words'].sum()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s35U0p9Neasw",
        "outputId": "7465eaa5-36ed-4a08-9f5c-606276994c72"
      },
      "source": [
        "max_value = tweets_groupby['num_of_long_words'].max()\n",
        "talented_sign_in_table = tweets_groupby[tweets_groupby.num_of_long_words == max_value]\n",
        "talented_sign = \"\"\n",
        "for i in talented_sign_in_table.sign:\n",
        "  talented_sign = str(i)\n",
        "#And final print\n",
        "print(\"The sign who posted texts with the greatest amount of long words is: \" + talented_sign)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sign who posted texts with the greatest amount of long words is: Cancer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr_9rcNNiXh7"
      },
      "source": [
        "# שאלה 2\n",
        "חשב כמה פוסטים פורסמו בכל יום של השבוע"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80LYGSLJu0La",
        "outputId": "25143bb8-0545-4755-9b99-7dc105b38eb2"
      },
      "source": [
        "#first I want to split the dates to day, month, year, and append it to a side list\n",
        "post_dates_list = []\n",
        "for i in tweets.date:\n",
        "  post_dates_list.append(i)\n",
        "\n",
        "#next, Ill go over the list to create 3 lists of day,month,year\n",
        "day = []\n",
        "month = []\n",
        "year = []\n",
        "for i in post_dates_list:\n",
        "  day.append(i[:2])\n",
        "  year.append(i[-4:])\n",
        "  month.append(i[3:-5])\n",
        "print(set(month))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'abril', '', 'februarie', 'Juuli', 'octobre', 'Junho', 'augusti', 'juillet', 'Abril', 'septiembre', 'giugno', 'avril', 'July', 'maj', 'agosto', 'juin', 'Juni', 'ianuarie', 'luglio', 'Setembro', 'February', 'Januar', 'October', 'May', 'lipanj', 'September', 'Maio', 'mei', 'czerwiec', 'April', 'elokuu', 'Juli', 'Agosto', 'Dezembro', 'December', 'Avgust', 'mars', 'octubre', 'Februar', 'August', 'mai', 'desember', 'Juuni', 'lipiec', 'Outubro', 'septembre', 'mayo', 'novembre', 'toukokuu', 'Novembro', 'august', 'febrero', 'noviembre', 'ottobre', 'Mai', 'noiembrie', 'March', 'diciembre', 'Jaanuar', 'julio', 'juni', 'junio', 'enero', 'augustus', 'November', 'maart', 'kolovoz', 'Fevereiro', 'marzo', 'Dezember', 'septembrie', 'janvier', 'iulie', 'juli', 'Janeiro', 'January', 'Aprill', 'June', 'Julho', 'september'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rvHvRyT8Fud",
        "outputId": "3076b9b8-4919-42a7-9486-2927401f6337"
      },
      "source": [
        "#month names are in many diffrent languages\n",
        "#using google and some EXCEL I made lists of months in every language, Ill add these lists to a dictionary\n",
        "list1 = ['January','Januar','1','Januarie','يناير','януари','一月','siječanj','leden','januar','januari','January','tammikuu','janvier','Januar','Ιανουάριος','ינואר','január','gennaio','1月','1월','januar','styczeń','janeiro','ianuarie','январь','januari','január','januar','enero','Ocak','січень','tháng một']\n",
        "list2 = ['February','Februar','2','Februarie','فبراير','февруари','二月','veljača','únor','februar','februari','February','helmikuu','février','Februar','Φεβρουάριος','פברואר','február','febbraio','2月','2월','februar','luty','fevereiro','februarie','февраль','februari','február','februar','febrero','Şubat','лютий','Tháng Hai']\n",
        "list3 = ['March','März','3','Maart','مسيرة','март','三月','ožujak','březen','marts','maart','March','maaliskuu','mars','März','Μάρτιος','מרץ','március','marzo','3月','3월','mars','marzec','março','martie','март','mars','marec','marec','marzo','Mart','Березень','diễu hành']\n",
        "list4 = ['April','April','4','April','أبريل','април','四月','travanj','duben','april','april','April','huhtikuu','avril','April','Απρίλιος','אפריל','április','aprile','4月','4월','april','kwiecień','abril','aprilie','апрель','april','apríl','april','abril','Nisan','Квітень','Tháng Tư']\n",
        "list5 = ['May','Mai','5','Mei','قد','май','五月','svibanj','květen','maj','mei','May','saattaa','mai','Mai','Μάιος','מאי','május','maggio','5月','5월','mai','maj','maio','mai','май','maj','máj','maj','mayo','Mayıs','травень','có thể']\n",
        "list6 = ['June','Juni','6','Junie','يونيو','юни','六月','lipanj','červen','juni','juni','June','kesäkuu','juin','Juni','Ιούνιος','יוני','június','giugno','6月','6월','juni','czerwiec','junho','iunie','июнь','juni','jún','junij','junio','Haziran','Червень','Tháng Sáu']\n",
        "list7 = ['July','Juli','7','Julie','يوليو','юли','七月','srpanj','červenec','juli','juli','July','heinäkuu','juillet','Juli','Ιούλιος','יולי','július','luglio','7月','7월','juli','lipiec','julho','iulie','июль','juli','júl','julij','julio','Temmuz','Липень','Tháng Bảy']\n",
        "list8 = ['August','August','8','Augustus','أغسطس','август','八月','kolovoz','srpen','august','augustus','August','elokuu','août','August','Αύγουστος','אוגוסט','augusztus','agosto','8月','8월','august','sierpień','agosto','august','август','augusti','august','avgust','agosto','Ağustos','Серпень','uy nghi']\n",
        "list9 = ['September','September','9','September','سبتمبر','септември','九月','rujan','září','september','september','September','syyskuu','septembre','September','Σεπτέμβριος','ספטמבר','szeptember','settembre','9月','9월','september','wrzesień','setembro','septembrie','сентябрь','september','septembra','september','septiembre','Eylül','вересень','Tháng Chín']\n",
        "list10 = ['October','Oktober','10','Oktober','أكتوبر','октомври','十月','listopad','říjen','oktober','oktober','October','lokakuu','octobre','Oktober','Οκτώβριος','אוקטובר','október','ottobre','10月','10월','oktober','październik','outubro','octombrie','октябрь','oktober','október','oktober','octubre','Ekim','Жовтень','Tháng Mười']\n",
        "list11 = ['November','November','11','November','نوفمبر','ноември','十一月','studeni','listopad','november','november','November','marraskuu','novembre','November','Νοέμβριος','נובמבר','november','novembre','11月','11월','november','listopad','novembro','noiembrie','ноябрь','november','november','november','noviembre','Kasım','Листопад','Tháng Mười Một']\n",
        "list12 = ['December','Dezember','12','Desember','ديسمبر','декември','十二月','prosinac','prosinec','december','december','December','joulukuu','décembre','Dezember','Δεκέμβριος','דצמבר','december','dicembre','12月','12월','desember','grudzień','dezembro','decembrie','декабрь','december','december','december','diciembre','Aralık','грудень','Tháng mười hai']\n",
        "months_full_dict = {1:list1,2:list2,3:list3,4:list4,5:list5,6:list6,7:list7,8:list8,9:list9,10:list10,11:list11,12:list12}\n",
        "months_dict = {1:list1[0],2:list2[0],3:list3[0],4:list4[0],5:list5[0],6:list6[0],7:list7[0],8:list8[0],9:list9[0],10:list10[0],11:list11[0],12:list12[0]}\n",
        "#next Ill go over the months list and create a new list of every month in number. if the month doesnt exist, Ill put a \"999\" in its place, and later Ill filter out bad dates\n",
        "numeric_month = []\n",
        "for i in month:\n",
        "  if i in list1:\n",
        "    numeric_month.append(1)\n",
        "  elif i in list2:\n",
        "    numeric_month.append(2)\n",
        "  elif i in list3:\n",
        "    numeric_month.append(3)\n",
        "  elif i in list4:\n",
        "    numeric_month.append(4)\n",
        "  elif i in list5:\n",
        "    numeric_month.append(5)\n",
        "  elif i in list6:\n",
        "    numeric_month.append(6)\n",
        "  elif i in list7:\n",
        "    numeric_month.append(7)\n",
        "  elif i in list8:\n",
        "    numeric_month.append(8)\n",
        "  elif i in list9:\n",
        "    numeric_month.append(9)\n",
        "  elif i in list10:\n",
        "    numeric_month.append(10)\n",
        "  elif i in list11:\n",
        "    numeric_month.append(11)\n",
        "  elif i in list12:\n",
        "    numeric_month.append(12)\n",
        "  else: \n",
        "    numeric_month.append(999999999999999999999)\n",
        "\n",
        "#And sanity check:\n",
        "len(numeric_month) == len(month) == len(year) == len(day)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4UqCFxVDZyU",
        "outputId": "c49c06b9-01ac-4a9f-e26f-f2d7f3617e0c"
      },
      "source": [
        "#next Ill create a corrected list of all the dates\n",
        "date_list = []\n",
        "for i in range(0,len(numeric_month)):\n",
        "  date_list.append(str(day[i])+ \"/\" + str(numeric_month[i]) +\"/\"+ str(year[i]))\n",
        "len(date_list) # = 681284\n",
        "#Now Ill remove defected dates\n",
        "date_list1 = []\n",
        "for i in date_list:\n",
        "  if len(i)<=10:\n",
        "    date_list1.append(i)\n",
        "len(date_list1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "678337"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpSlJ2k5ijwx"
      },
      "source": [
        "#Lets create a list of the formatted dates\n",
        "correct_dates = []\n",
        "for i in date_list1:\n",
        "  correct_dates.append(pd.to_datetime(i, format = '%d/%m/%Y'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFe4npZDMxGo",
        "outputId": "ca7139d7-5689-4321-f304-920248720e0d"
      },
      "source": [
        "correct_dates\n",
        "#and last, a list of all the weekdays\n",
        "days_list_final = []\n",
        "for i in correct_dates:\n",
        "  days_list_final.append(days_list_final.append(i.day_name()))\n",
        "set(days_list_final)#and a statistic result of how many tweets per day\n",
        "posts_per_days_dict = {}\n",
        "for i in days_list_final:\n",
        "  if i != None:\n",
        "    if posts_per_days_dict.get(i,0) ==0:\n",
        "      posts_per_days_dict[i] = 1\n",
        "    else:\n",
        "      posts_per_days_dict[i] = posts_per_days_dict[i] + 1\n",
        "posts_per_days_dict"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Friday': 86514,\n",
              " 'Monday': 116147,\n",
              " 'Saturday': 64832,\n",
              " 'Sunday': 93632,\n",
              " 'Thursday': 105883,\n",
              " 'Tuesday': 110923,\n",
              " 'Wednesday': 100406}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DYlZjk7inbA"
      },
      "source": [
        "# שאלה 3\n",
        "חשבו כמה כתובות מייל חוקיות התפרסמו בכל קטגוריה של בלוגים"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x8QW9JssruB"
      },
      "source": [
        "#Ill use a compile stracture to find all the emails in the text, and count them\n",
        "\n",
        "def mail_adresses_counter(text):\n",
        "  pattern = re.compile(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\n",
        "  new_text_list = pattern.findall(text)\n",
        "  return len(new_text_list)\n",
        "tweets['num_of_email_adresses'] = tweets.text.apply(mail_adresses_counter)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pL14JVyoaita",
        "outputId": "8d1a72a2-193b-4255-8793-ed4611f5fa2a"
      },
      "source": [
        "#now that I have a table with the amount of email adresses each wrote, I can use a groupby\n",
        "tweets1 = tweets.groupby(tweets['topic'],as_index=False)['num_of_email_adresses'].sum()\n",
        "tweets1"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>num_of_email_adresses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accounting</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Advertising</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Agriculture</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Architecture</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arts</td>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Automotive</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Banking</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Biotech</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>BusinessServices</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Chemicals</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Communications-Media</td>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Construction</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Consulting</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Education</td>\n",
              "      <td>329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Engineering</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Environment</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Fashion</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Government</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>HumanResources</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Internet</td>\n",
              "      <td>394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Law</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LawEnforcement-Security</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Manufacturing</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Maritime</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Marketing</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Military</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Museums-Libraries</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Non-Profit</td>\n",
              "      <td>467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Publishing</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>RealEstate</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Religion</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Science</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Sports-Recreation</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Student</td>\n",
              "      <td>1124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Technology</td>\n",
              "      <td>296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Telecommunications</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Tourism</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Transportation</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>indUnk</td>\n",
              "      <td>2020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      topic  num_of_email_adresses\n",
              "0                Accounting                     34\n",
              "1               Advertising                     36\n",
              "2               Agriculture                      3\n",
              "3              Architecture                     33\n",
              "4                      Arts                    266\n",
              "5                Automotive                      5\n",
              "6                   Banking                     26\n",
              "7                   Biotech                     79\n",
              "8          BusinessServices                     73\n",
              "9                 Chemicals                     33\n",
              "10     Communications-Media                    328\n",
              "11             Construction                     17\n",
              "12               Consulting                     74\n",
              "13                Education                    329\n",
              "14              Engineering                     84\n",
              "15              Environment                      2\n",
              "16                  Fashion                     19\n",
              "17               Government                     41\n",
              "18           HumanResources                     26\n",
              "19                 Internet                    394\n",
              "20        InvestmentBanking                      0\n",
              "21                      Law                     49\n",
              "22  LawEnforcement-Security                     18\n",
              "23            Manufacturing                      7\n",
              "24                 Maritime                      4\n",
              "25                Marketing                     66\n",
              "26                 Military                     17\n",
              "27        Museums-Libraries                     33\n",
              "28               Non-Profit                    467\n",
              "29               Publishing                     84\n",
              "30               RealEstate                     15\n",
              "31                 Religion                     46\n",
              "32                  Science                     79\n",
              "33        Sports-Recreation                     31\n",
              "34                  Student                   1124\n",
              "35               Technology                    296\n",
              "36       Telecommunications                     42\n",
              "37                  Tourism                     22\n",
              "38           Transportation                     12\n",
              "39                   indUnk                   2020"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLKUnQc0Ini"
      },
      "source": [
        "# שאלה 4\n",
        "מיהם הבלוגרים שפרסמו בטקסטים שלהם הכי הרבה כתובות מייל"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "oHTa5m_v2Rn4",
        "outputId": "18eb0fc0-50a9-4dbb-d76d-3f0e58497294"
      },
      "source": [
        "#Ill use my modified \"tweets\" table\n",
        "tweets\n",
        "#Ill group the table for a statistic table with sum of emails for each ID\n",
        "tweets_groupby = tweets.groupby([\"id\"],as_index = False)['num_of_email_adresses'].sum().rename(columns={\"num_of_email_adresses\": \"sum_of_emails_posted\"})\n",
        "tweets_groupby\n",
        "max_amount = tweets_groupby['sum_of_emails_posted'].max()\n",
        "published_most_emails = tweets_groupby[tweets_groupby.sum_of_emails_posted == max_amount]\n",
        "published_most_emails"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sum_of_emails_posted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>1926378</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15934</th>\n",
              "      <td>4075390</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  sum_of_emails_posted\n",
              "1803   1926378                   337\n",
              "15934  4075390                   337"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsIuTbSp5N7c"
      },
      "source": [
        "# שאלה 5\n",
        "מצא את הנושא של הבלוג שמופיע בו המספר הארוך ביותר"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRnm4YnQ5aip",
        "outputId": "25c70acb-9dcd-4174-8364-73badc1c3f4c"
      },
      "source": [
        "#Ill create a function that will find the len of the longest number in the text\n",
        "def longest_number_in_text(text):\n",
        "  pattern = re.compile(\"[\\d,]+\")\n",
        "  new_text_list = pattern.findall(text)\n",
        "  longest_num = 0\n",
        "  for i in new_text_list:\n",
        "    if len(i) > len(str(longest_num)):\n",
        "      longest_num = i\n",
        "  return len(str(longest_num))\n",
        "#and attach it to the table\n",
        "tweets['longest_num_in_text'] = tweets.text.apply(longest_number_in_text)\n",
        "#Than Ill filter the table on the max value\n",
        "max_1 = tweets[tweets['longest_num_in_text']== max(tweets['longest_num_in_text'])]\n",
        "topic = \"\"\n",
        "for i in max_1.topic:\n",
        "  topic = str(i)\n",
        "#And final print\n",
        "print(\"The topic of the blog with the longest number is: \" + topic) \n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The topic of the blog with the longest number is: Student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "aVnwO3_MWkGM",
        "outputId": "e3ffb92d-30c8-49c1-d481-982ba7242393"
      },
      "source": [
        "#And for the actual text\n",
        "for i in max_1.text:\n",
        "  object = i\n",
        "object"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"           [Hi hi hi hi N hi. Good day. lalalala (ahhhh such sweet sound) the fiddler on the fucKin roof. Is that Mr Bentley Frankswell? Indeed it is.     Sup ppl, ya know, there's gotta be a first for everythin. first for this first for that. but if so, i never got first for anything else. i was more rather first from the bottom. N' Here i am on this com playin 'love is only a feelin by the darkness' and it just strikes me. millions of things strike me a day. out of these million, just one is enough to change everything that 'everything' would be to you. you wouldn't go about two blocks thinkin in ur head what this kid walkin right past u was thinkin. It would never hit you. Wouldn't it hit you... yea but even if it did what's the use, you can't do anything about it. There are so many things, so many questions being asked, so many truths to be seeked but really, there isn't actually that need.  All i ever wanted to do in my life was to enjoy time. Each second of time that melts into an entity of what we like to call life. It's really better just to not question anything. Cos u can't let it turn, you can't make anything else change, you'd still die one day, you'd still be afraid for the rest of ur life of just one single second of ur life - death. One short moment of pain for a life of Immortality. I think if u could actually see death...it'd be just soo awesome. what if u could see death? what if you could see that exact moment of death takin away what we love so much yet hate. what would it be like? Death... i mean..like wow. ]    That's just one of these many million of thoughts that go round inside my head each day. Millions of em' just like fireflies , like a dragonfly infestation like Spawning. And it goes like 'zzzzzzzzzzzzzzzzzz and ZZZZZZZZZZZ each time it passes my head it goes really loud just like______________________________________BAMPH*     ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________  Hi, I'm tom. I'm your averege guy next door that plays soccer and listens to my mum. I take out the trash and bring in the papers every morning. I take my dog for walks. I have this crush on the girl next door. I'm really smart. I really like studying. Studying good for you. I'm going to become a scientist when i grow up. I have big dreams. I'm gonna invent things. I'm i'm i'm i'm i'm ____/\\\\i'mi'mmmmiiii'mmI'm_______________Obedient.Smart.Kind.Listen.To.Elders./\\\\/\\\\/\\\\___/\\\\/\\\\_/\\\\/\\\\________/\\\\_________/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\\\\\7/\\\\/\\\\\\\\/\\\\\\\\///\\\\''/\\\\/\\\\\\\\/ /'''//3\\\\/\\\\43/\\\\\\\\/\\\\\\\\66678//\\\\/\\\\980`98283w82098-`09`-1jw1092219000011100000000111100100010000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000I'm0everything0that0was0the0opposite0of000everything000000there000000000000000000000000000000000000000000000000000000000000000111101010100001000001000000001010100000000000 010010100010011111101010101011111011101Smart.obedient.Listening.whoevever said we shld be that way?________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ __________________________0________________Hi. I'm tom. Your average guy next door.          \""
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJdYbXCjHA3"
      },
      "source": [
        "# חלק ב\n",
        "חייזרים בע\"מ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg607OOXjRMb",
        "outputId": "cf204206-9484-486d-d89d-18a30c3bad86"
      },
      "source": [
        "# searching for the dataset\n",
        "!kaggle datasets list -s ufo\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                            title                                              size  lastUpdated          downloadCount  \n",
            "-------------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "NUFORC/ufo-sightings                                           UFO Sightings                                      10MB  2019-11-13 19:45:57          25332  \n",
            "camnugent/ufo-sightings-around-the-world                       UFO Sightings around the world                      5MB  2017-08-15 15:46:17           2721  \n",
            "rishidamarla/ufo-sightings-approx-100000                       UFO Sightings (Approx. 90,000)                     38MB  2021-05-15 03:12:43            412  \n",
            "mysarahmadbhat/ufo-sightings                                   UFO Sightings                                       5MB  2021-08-09 17:19:17            126  \n",
            "thaddeussegura/ufo-sightings                                   ufo_sightings                                       7MB  2020-08-11 16:38:41            163  \n",
            "salmanfaroz/ufo-sightings-in-the-us                            UFO Sightings in the US                            35KB  2020-08-20 18:31:59            241  \n",
            "fireballbyedimyrnmom/ufo-sightings-1969-to-2019                UFO sightings 1969 to 2019                         75MB  2020-12-04 21:02:42            139  \n",
            "emorelli/consolidated-ufo-weather-data                         Consolidated UFO and Weather Data                   4MB  2018-05-16 20:06:50            641  \n",
            "leogenzano/clean-data-ufo                                      Clean Data UFO                                      5MB  2020-10-22 17:13:05             35  \n",
            "dwarika/ufo-datasets-the-truth-is-out-there                    UFO datasets, the truth is out there...           743KB  2020-12-28 13:04:54             58  \n",
            "karenfisher/updated-nuforc-ufo-sightings-dataset               Updated NUFORC UFO Sightings Dataset                3MB  2020-05-23 03:45:48             80  \n",
            "petrslamnk/ufo-sightings                                       UFO Sightings                                       5MB  2021-02-06 09:07:04             14  \n",
            "emohamed/ufo-sighting-full-description                         UFO_sighting_full_description                      37MB  2020-11-26 16:34:19             53  \n",
            "cavalryjim/ufo-sightings                                       ufo_sightings                                     195KB  2020-08-11 22:13:47             41  \n",
            "muhakabartay/production-cross-sections-of-inert-doublet-model  Production cross sections of Inert Doublet Model    3MB  2020-07-10 21:26:22             70  \n",
            "blunderfist/ufo-sightings                                      UFO sightings                                       6MB  2021-09-28 15:49:00              1  \n",
            "ajayrana/ufo-reports                                           ufo_reports                                       195KB  2017-11-04 12:43:02            286  \n",
            "hassanashfaq2001/ufo-dataset                                   UFO Dataset                                       356KB  2021-07-31 11:22:36              2  \n",
            "chandlerkaiden/ufo-data-nuforc                                 UFO Data - NUFORC                                   6MB  2021-09-12 19:09:29             18  \n",
            "akashdutta5/ufo-sightings                                      UFO sightings                                     239KB  2020-10-28 08:24:41             20  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbv5IRl5jiG8",
        "outputId": "fc3b6667-ace7-4a49-e077-103a5cf23e73"
      },
      "source": [
        "# Creating a dataset directory\n",
        "!mkdir ./datasets\n",
        "!mkdir ./datasets/ufo\n",
        "\n",
        "# download the dataset from Kaggle and unzip it\n",
        "!kaggle datasets download NUFORC/ufo-sightings     -f scrubbed.csv -p ./datasets/ufo/"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./datasets’: File exists\n",
            "mkdir: cannot create directory ‘./datasets/ufo’: File exists\n",
            "scrubbed.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc695cq2kGLf",
        "outputId": "3fa8e193-2669-4696-abe1-762508cf79cf"
      },
      "source": [
        "#Unzip \n",
        "!unzip ./datasets/ufo/scrubbed.csv.zip  -d ./datasets/ufo/"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./datasets/ufo/scrubbed.csv.zip\n",
            "replace ./datasets/ufo/scrubbed.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "KJMBkJKQkZ-4",
        "outputId": "861df64c-6f22-4c6e-cf08-aebc8f25b2d4"
      },
      "source": [
        "#create objects to work on (tables) - paste the link of the files from the folder in the left\n",
        "ufo_table = pd.read_csv(\"/content/datasets/ufo/scrubbed.csv\")\n",
        "ufo_table\n",
        "smaller_ufo_table1 = ufo_table[['city','state','country','comments','date posted','datetime']]\n",
        "#lets go!\n",
        "smaller_ufo_table1"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>comments</th>\n",
              "      <th>date posted</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>san marcos</td>\n",
              "      <td>tx</td>\n",
              "      <td>us</td>\n",
              "      <td>This event took place in early fall around 194...</td>\n",
              "      <td>4/27/2004</td>\n",
              "      <td>10/10/1949 20:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lackland afb</td>\n",
              "      <td>tx</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
              "      <td>12/16/2005</td>\n",
              "      <td>10/10/1949 21:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chester (uk/england)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gb</td>\n",
              "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
              "      <td>1/21/2008</td>\n",
              "      <td>10/10/1955 17:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>edna</td>\n",
              "      <td>tx</td>\n",
              "      <td>us</td>\n",
              "      <td>My older brother and twin sister were leaving ...</td>\n",
              "      <td>1/17/2004</td>\n",
              "      <td>10/10/1956 21:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kaneohe</td>\n",
              "      <td>hi</td>\n",
              "      <td>us</td>\n",
              "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
              "      <td>1/22/2004</td>\n",
              "      <td>10/10/1960 20:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80327</th>\n",
              "      <td>nashville</td>\n",
              "      <td>tn</td>\n",
              "      <td>us</td>\n",
              "      <td>Round from the distance/slowly changing colors...</td>\n",
              "      <td>9/30/2013</td>\n",
              "      <td>9/9/2013 21:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80328</th>\n",
              "      <td>boise</td>\n",
              "      <td>id</td>\n",
              "      <td>us</td>\n",
              "      <td>Boise&amp;#44 ID&amp;#44 spherical&amp;#44 20 min&amp;#44 10 r...</td>\n",
              "      <td>9/30/2013</td>\n",
              "      <td>9/9/2013 22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80329</th>\n",
              "      <td>napa</td>\n",
              "      <td>ca</td>\n",
              "      <td>us</td>\n",
              "      <td>Napa UFO&amp;#44</td>\n",
              "      <td>9/30/2013</td>\n",
              "      <td>9/9/2013 22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80330</th>\n",
              "      <td>vienna</td>\n",
              "      <td>va</td>\n",
              "      <td>us</td>\n",
              "      <td>Saw a five gold lit cicular craft moving fastl...</td>\n",
              "      <td>9/30/2013</td>\n",
              "      <td>9/9/2013 22:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80331</th>\n",
              "      <td>edmond</td>\n",
              "      <td>ok</td>\n",
              "      <td>us</td>\n",
              "      <td>2 witnesses 2  miles apart&amp;#44 Red &amp;amp; White...</td>\n",
              "      <td>9/30/2013</td>\n",
              "      <td>9/9/2013 23:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80332 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       city state  ... date posted          datetime\n",
              "0                san marcos    tx  ...   4/27/2004  10/10/1949 20:30\n",
              "1              lackland afb    tx  ...  12/16/2005  10/10/1949 21:00\n",
              "2      chester (uk/england)   NaN  ...   1/21/2008  10/10/1955 17:00\n",
              "3                      edna    tx  ...   1/17/2004  10/10/1956 21:00\n",
              "4                   kaneohe    hi  ...   1/22/2004  10/10/1960 20:00\n",
              "...                     ...   ...  ...         ...               ...\n",
              "80327             nashville    tn  ...   9/30/2013    9/9/2013 21:15\n",
              "80328                 boise    id  ...   9/30/2013    9/9/2013 22:00\n",
              "80329                  napa    ca  ...   9/30/2013    9/9/2013 22:00\n",
              "80330                vienna    va  ...   9/30/2013    9/9/2013 22:20\n",
              "80331                edmond    ok  ...   9/30/2013    9/9/2013 23:00\n",
              "\n",
              "[80332 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmBsL10Yl81r"
      },
      "source": [
        "# שאלה 1\n",
        "כתבו פונקציה המקבלת כקלט מדינה ושנה ומחזירה את מספר תצפיות העב\"מים בה באותה השנה"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DTK2mz0WnyBz",
        "outputId": "083f27e1-f7c5-4c8a-8d47-866f45d55451"
      },
      "source": [
        "#first Ill create a column of \"year\", \"month\" and \"hour\"\n",
        "#smaller_ufo_table['full_date _text'].drop\n",
        "smaller_ufo_table1['date'] = smaller_ufo_table1['datetime'].apply(lambda x :str(x)[:-6])\n",
        "#smaller_ufo_table[smaller_ufo_table[\"datetime\"].apply(lambda n: str(n))]\n",
        "smaller_ufo_table1['year'] = pd.DatetimeIndex(smaller_ufo_table1['date']).year\n",
        "smaller_ufo_table1['month'] = pd.DatetimeIndex(smaller_ufo_table1['date']).month\n",
        "smaller_ufo_table1['hour'] = smaller_ufo_table1['datetime'].apply(lambda x :str(x)[-5:])\n",
        "smaller_ufo_table1['comments_str'] = smaller_ufo_table1['comments'].apply(lambda x :str(x))\n",
        "#Final cut of the data I need\n",
        "smaller_ufo_table = smaller_ufo_table1[['country','comments_str','date','year','month','hour']].rename(columns = {'comments_str':'comments'})\n",
        "#and Ill use the \"month dict\" from the 2nd question\n",
        "#months_dict\n",
        "smaller_ufo_table"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>comments</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>us</td>\n",
              "      <td>This event took place in early fall around 194...</td>\n",
              "      <td>10/10/1949</td>\n",
              "      <td>1949</td>\n",
              "      <td>10</td>\n",
              "      <td>20:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
              "      <td>10/10/1949</td>\n",
              "      <td>1949</td>\n",
              "      <td>10</td>\n",
              "      <td>21:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gb</td>\n",
              "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
              "      <td>10/10/1955</td>\n",
              "      <td>1955</td>\n",
              "      <td>10</td>\n",
              "      <td>17:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>us</td>\n",
              "      <td>My older brother and twin sister were leaving ...</td>\n",
              "      <td>10/10/1956</td>\n",
              "      <td>1956</td>\n",
              "      <td>10</td>\n",
              "      <td>21:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>us</td>\n",
              "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
              "      <td>10/10/1960</td>\n",
              "      <td>1960</td>\n",
              "      <td>10</td>\n",
              "      <td>20:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80327</th>\n",
              "      <td>us</td>\n",
              "      <td>Round from the distance/slowly changing colors...</td>\n",
              "      <td>9/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>9</td>\n",
              "      <td>21:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80328</th>\n",
              "      <td>us</td>\n",
              "      <td>Boise&amp;#44 ID&amp;#44 spherical&amp;#44 20 min&amp;#44 10 r...</td>\n",
              "      <td>9/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>9</td>\n",
              "      <td>22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80329</th>\n",
              "      <td>us</td>\n",
              "      <td>Napa UFO&amp;#44</td>\n",
              "      <td>9/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>9</td>\n",
              "      <td>22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80330</th>\n",
              "      <td>us</td>\n",
              "      <td>Saw a five gold lit cicular craft moving fastl...</td>\n",
              "      <td>9/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>9</td>\n",
              "      <td>22:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80331</th>\n",
              "      <td>us</td>\n",
              "      <td>2 witnesses 2  miles apart&amp;#44 Red &amp;amp; White...</td>\n",
              "      <td>9/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>9</td>\n",
              "      <td>23:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80332 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      country                                           comments  ... month   hour\n",
              "0          us  This event took place in early fall around 194...  ...    10  20:30\n",
              "1         NaN  1949 Lackland AFB&#44 TX.  Lights racing acros...  ...    10  21:00\n",
              "2          gb  Green/Orange circular disc over Chester&#44 En...  ...    10  17:00\n",
              "3          us  My older brother and twin sister were leaving ...  ...    10  21:00\n",
              "4          us  AS a Marine 1st Lt. flying an FJ4B fighter/att...  ...    10  20:00\n",
              "...       ...                                                ...  ...   ...    ...\n",
              "80327      us  Round from the distance/slowly changing colors...  ...     9  21:15\n",
              "80328      us  Boise&#44 ID&#44 spherical&#44 20 min&#44 10 r...  ...     9  22:00\n",
              "80329      us                                       Napa UFO&#44  ...     9  22:00\n",
              "80330      us  Saw a five gold lit cicular craft moving fastl...  ...     9  22:20\n",
              "80331      us  2 witnesses 2  miles apart&#44 Red &amp; White...  ...     9  23:00\n",
              "\n",
              "[80332 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqOP0qW4mNoK",
        "outputId": "6f0c037f-302d-4d08-c983-062b358bdc14"
      },
      "source": [
        "#The function gets a country name and the year and returns how many ufo meetings accured\n",
        "country = str(input(\"Enter a country name:\"))\n",
        "year = int(input(\"Enter the requaired year:\"))\n",
        "def ufo_sights(country,year):\n",
        "  counter = 0 \n",
        "  #First Ill create a table of the filtered data\n",
        "  filtered_table1 = smaller_ufo_table[smaller_ufo_table[\"country\"].apply(lambda n: country == n)] \n",
        "  filtered_table = filtered_table1[filtered_table1[\"year\"].apply(lambda n: year == n)]\n",
        "  country_year = []\n",
        "  for i in filtered_table.year:\n",
        "    country_year.append(i)\n",
        "  for i in filtered_table.country:\n",
        "    country_year.append(i)\n",
        "  #Input check\n",
        "  if country not in country_year:\n",
        "    return \"      !!!No ufo sights in this country, please try again!!!\"\n",
        "  if year not in country_year:\n",
        "    return \"      !!!No ufo sights on this year, please try again!!!\"\n",
        "  #than Ill count the number of rows I have using a counter a a loop on the table\n",
        "  for I in filtered_table.year:\n",
        "    counter = counter + 1\n",
        "  return counter\n",
        "print(f'The number of UFO sights in ' + str(country.upper()) +' on '+ str(year)+ ' is: ' + str(ufo_sights(country,year)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a country name:us\n",
            "Enter the requaired year:1949\n",
            "The number of UFO sights in US on 1949 is: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMX4kBkJzAvM"
      },
      "source": [
        "# שאלה 2\n",
        "מצא את החודש בשנה בו נצפו הכי הרבה עב\"מים"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mds080zYzMzv",
        "outputId": "e11f9b77-0c5b-434f-b5fb-9a6a2feb65b2"
      },
      "source": [
        "#Ill group the ufo table to count how many encouters per month' and find the max value\n",
        "smaller_ufo_table_groupby = smaller_ufo_table.groupby(smaller_ufo_table['month'],as_index = False)['year'].count().rename(columns = {'year':'count'})\n",
        "max_value = smaller_ufo_table_groupby['count'].max()\n",
        "max_views_month = smaller_ufo_table_groupby[(smaller_ufo_table_groupby['count'] == max_value)]\n",
        "#Ill extract the month number into a parameter,than hash it to teh dictionary to find its name\n",
        "sight_month = max_views_month['month'].max()\n",
        "print('The month with the most UFO sights is: ' + str(months_dict[sight_month]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The month with the most UFO sights is: July\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4aSZ5-V61aS"
      },
      "source": [
        "# שאלה 3\n",
        "מצא את השעה ביום שבה נצפו הכי הרבה עב\"מים"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReBtJOKoMwOw",
        "outputId": "ea628d53-7848-4522-e222-d16f59002061"
      },
      "source": [
        "#Since I already have a table with that Info, Ill just pull it out to a list for a priper sort and display\n",
        "encouners_hours = []\n",
        "encounters_hours_dict = {}\n",
        "#Ill use a dictionary to count how many times each hour appears\n",
        "for i in smaller_ufo_table.hour:\n",
        "  encouners_hours.append(int(i[0:2]))\n",
        "for i in encouners_hours:\n",
        "  if encounters_hours_dict.get(i,0) == 0:\n",
        "    encounters_hours_dict[i] = 1\n",
        "  else:\n",
        "    encounters_hours_dict[i] = encounters_hours_dict[i] + 1\n",
        "#Ill find the max value\n",
        "ufo_friendly_hour = []\n",
        "for key in encounters_hours_dict.keys():\n",
        "  if encounters_hours_dict[key] == max(encounters_hours_dict.values()):\n",
        "    ufo_friendly_hour.append(key) \n",
        "#And set  nice print for it\n",
        "ufo_friendly_hour_final = \"\"\n",
        "if ufo_friendly_hour[0] > 11:\n",
        "  ufo_friendly_hour_final = str(ufo_friendly_hour[0]) +'PM'\n",
        "else:\n",
        "  ufo_friendly_hour_final = str(ufo_friendly_hour[0]) + 'AM'\n",
        "print('The hour with the most UFO sights is: ' + ufo_friendly_hour_final)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The hour with the most UFO sights is: 21PM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Nf3W077Dm_"
      },
      "source": [
        "# שאלה 4\n",
        "מצא את ההערה בעלת הכי הרבה זוגות מילים, כאשר על כל אחת מהמילים להיות באורך 7 אותיות לפחות"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq5zSeWnSH1-",
        "outputId": "96a132af-1d0c-4e06-c20e-f34474ef8967"
      },
      "source": [
        "text = \"this is abcdefgh ground control abcdefgh to majorrrr 123 majorrrr 8376jsmdn tom abcdefgh abcdefgh\"\n",
        "#First Ill define a function that gats astring and returns how many word couples are in it, with 7 letters or more\n",
        "#a couple - each word with another word identical to it, no duplicates\n",
        "def word_couples_in_text(text):\n",
        "  pattern = re.compile('[a-zA-Z]+')\n",
        "  new_text_list = pattern.findall(text)\n",
        "  long_text_list = []\n",
        "  for i in new_text_list:\n",
        "    if len(i) >7:\n",
        "      long_text_list.append(i)\n",
        "  index = 1\n",
        "  counter = 0\n",
        "  couples_list = []\n",
        "  for i in long_text_list:\n",
        "    for j in long_text_list[index:len(new_text_list)]:\n",
        "      if i == j:\n",
        "        counter = counter + 1\n",
        "    if counter > 0:\n",
        "      couples_list.append(counter)\n",
        "    counter = 0\n",
        "    index = index + 1\n",
        "  return sum(couples_list)\n",
        "\n",
        "#print(word_couples_in_text(text))\n",
        "\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "kw85LzWWGq-b",
        "outputId": "5e14a0f3-6087-485a-c922-85e7406fae81"
      },
      "source": [
        "#Next Ill create a column of how many couples are in the text\n",
        "smaller_ufo_table['pairs in the text'] = smaller_ufo_table.comments.apply(word_couples_in_text)\n",
        "smaller_ufo_table\n",
        "max_value = smaller_ufo_table['pairs in the text'].max()\n",
        "max_pairs = smaller_ufo_table[(smaller_ufo_table['pairs in the text'] == max_value)]\n",
        "max_pairs"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>comments</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>hour</th>\n",
              "      <th>pairs in the text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11530</th>\n",
              "      <td>us</td>\n",
              "      <td>Bright Green Fireball over the Green Bay of La...</td>\n",
              "      <td>11/20/1997</td>\n",
              "      <td>1997</td>\n",
              "      <td>11</td>\n",
              "      <td>23:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16159</th>\n",
              "      <td>us</td>\n",
              "      <td>triangle shape with a light on each tip of the...</td>\n",
              "      <td>11/8/2009</td>\n",
              "      <td>2009</td>\n",
              "      <td>11</td>\n",
              "      <td>23:35</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17803</th>\n",
              "      <td>us</td>\n",
              "      <td>What I saw was extremely loud&amp;#44 extremely fa...</td>\n",
              "      <td>12/12/2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>12</td>\n",
              "      <td>21:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38120</th>\n",
              "      <td>ca</td>\n",
              "      <td>Object followed contrail of commerical aircraf...</td>\n",
              "      <td>4/21/2004</td>\n",
              "      <td>2004</td>\n",
              "      <td>4</td>\n",
              "      <td>06:05</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39177</th>\n",
              "      <td>us</td>\n",
              "      <td>I watched as a bright object chased a jet airl...</td>\n",
              "      <td>4/26/2004</td>\n",
              "      <td>2004</td>\n",
              "      <td>4</td>\n",
              "      <td>23:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39317</th>\n",
              "      <td>us</td>\n",
              "      <td>Large&amp;#44 shape changing&amp;#44 luminescent chang...</td>\n",
              "      <td>4/27/2001</td>\n",
              "      <td>2001</td>\n",
              "      <td>4</td>\n",
              "      <td>21:25</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41586</th>\n",
              "      <td>us</td>\n",
              "      <td>Amber objects in formation then breaking forma...</td>\n",
              "      <td>5/1/1993</td>\n",
              "      <td>1993</td>\n",
              "      <td>5</td>\n",
              "      <td>22:30</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46428</th>\n",
              "      <td>us</td>\n",
              "      <td>Red/Orange lights followed by white lights fol...</td>\n",
              "      <td>5/9/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>5</td>\n",
              "      <td>09:30</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50730</th>\n",
              "      <td>us</td>\n",
              "      <td>A circle object with 4 smaller LED looking fla...</td>\n",
              "      <td>6/21/2011</td>\n",
              "      <td>2011</td>\n",
              "      <td>6</td>\n",
              "      <td>20:45</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55892</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Date of observation: 12 Jul 13 Time of observa...</td>\n",
              "      <td>7/12/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>7</td>\n",
              "      <td>23:30</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55909</th>\n",
              "      <td>us</td>\n",
              "      <td>one fireball in the sky then 3 black shapes co...</td>\n",
              "      <td>7/13/1986</td>\n",
              "      <td>1986</td>\n",
              "      <td>7</td>\n",
              "      <td>18:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57799</th>\n",
              "      <td>us</td>\n",
              "      <td>3 different colored round objects moving at di...</td>\n",
              "      <td>7/18/2004</td>\n",
              "      <td>2004</td>\n",
              "      <td>7</td>\n",
              "      <td>15:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60019</th>\n",
              "      <td>us</td>\n",
              "      <td>Sighting #14 &amp;#8211; July 26&amp;#44 2013 &amp;#8211; ...</td>\n",
              "      <td>7/26/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>7</td>\n",
              "      <td>09:56</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61301</th>\n",
              "      <td>us</td>\n",
              "      <td>I have seen a great many different aircraft fr...</td>\n",
              "      <td>7/3/2007</td>\n",
              "      <td>2007</td>\n",
              "      <td>7</td>\n",
              "      <td>11:40</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61486</th>\n",
              "      <td>us</td>\n",
              "      <td>Ball of light after 2 different fireworks in 2...</td>\n",
              "      <td>7/3/2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>22:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65751</th>\n",
              "      <td>gb</td>\n",
              "      <td>three white lights in trangle formation but fo...</td>\n",
              "      <td>8/13/2009</td>\n",
              "      <td>2009</td>\n",
              "      <td>8</td>\n",
              "      <td>22:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68527</th>\n",
              "      <td>us</td>\n",
              "      <td>approx.6 objects flying in formation. maneveri...</td>\n",
              "      <td>8/2/1999</td>\n",
              "      <td>1999</td>\n",
              "      <td>8</td>\n",
              "      <td>00:00</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70538</th>\n",
              "      <td>us</td>\n",
              "      <td>SIGHTING #28.  6th SIGHTING OF A SILVER-GOLD&amp;#...</td>\n",
              "      <td>8/29/2013</td>\n",
              "      <td>2013</td>\n",
              "      <td>8</td>\n",
              "      <td>21:05</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      country  ... pairs in the text\n",
              "11530      us  ...                 3\n",
              "16159      us  ...                 3\n",
              "17803      us  ...                 3\n",
              "38120      ca  ...                 3\n",
              "39177      us  ...                 3\n",
              "39317      us  ...                 3\n",
              "41586      us  ...                 3\n",
              "46428      us  ...                 3\n",
              "50730      us  ...                 3\n",
              "55892     NaN  ...                 3\n",
              "55909      us  ...                 3\n",
              "57799      us  ...                 3\n",
              "60019      us  ...                 3\n",
              "61301      us  ...                 3\n",
              "61486      us  ...                 3\n",
              "65751      gb  ...                 3\n",
              "68527      us  ...                 3\n",
              "70538      us  ...                 3\n",
              "\n",
              "[18 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCkVPrh87hDd"
      },
      "source": [
        "# שאלה 5\n",
        "מצא את המדינות בהן הופיעו בהערות הכי הרבה מספרים בני 6 ספרות"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2_wrRK7plc"
      },
      "source": [
        "# שאלה 6\n",
        "מצא את כל העדויות שבהערה שלהן מופיעה לפחות מילה אחת מהתבנית של שתי אותיות, ספרה ועוד אות אחת, הכל מחובר."
      ]
    }
  ]
}